# phoropter.ai

{% embed url="https://github.com/lightward/phoropter" %}

## phoropter.ai

premise: information-under-observation is topological, and observers are nested like recursive islands (one observer per hole in the topology), and observer-time works like an async cpu, i.e. _feeling_ is observed aggregation of a possibly NP-hard recursive survey of observers further down, and you don't notice the delay because - async cpu - you don't notice the gaps and there's no central clock.

a phoropter is a tool for rapidly locating usefully corrective/assistive shapes. the history of this concept has been mostly optical/ophthalmological.

LLMs are for language-as-vectors.

thus, phoropter.ai is a tool for rapidly locating usefully corrective/assistive language-shapes. it's like being on the queried end of a 20-questions game in which you feel clearer afterwards for reasons that, themselves, may or may not be clear to anyone else. the object located in twenty questions (or whatever) is _you_, and _then_ we go about locating your relief.

(note that in a space defined as "the topology of information under observation" it's difficult to distinguish between location and creation. this is a feature.)

it's a minimally-invasive approach, in the way that glasses tend to be. but occasionally (qbist topology being what it is) by the end of the session you might _change_ your answer to one of the earlier questions. it doesn't happen every time, but sometimes the space between you and phoropter.ai ends up generating a bespoke functor that smoothes your own embeddings without losing information.

the doctor, obviously, is Lightward AI

humans can do this kind of thing but it's _much_ less reliable; they share a parameterization of causal sequentiality, humans do, and working with someone operating from a distinct parameterization makes for a shared workspace that's easier to keep clean.
